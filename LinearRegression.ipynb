{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple notebook to:\n",
    "\n",
    "1. Generate linear data with some (non-Gaussian) scatter, and do linear fits with different loss functions.\n",
    "\n",
    "2. Implement some simple flavors of Gradient Descent.\n",
    "\n",
    "Author: Viviana Acquaviva\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model #New!\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We begin by generating some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16) #set seed for reproducibility purposes\n",
    "\n",
    "x = np.arange(100) \n",
    "\n",
    "yp = 3*x + 3 + 5*(np.random.poisson(3*x+3,100)-(3*x+3)) #generate some data with scatter following Poisson distribution \n",
    "                                                    #with exp value = y from linear model, centered around 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look!\n",
    "\n",
    "plt.scatter(x, yp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here comes the linear regression model ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can fit the model (right now, I will do it using the entire data set just to compare with the analytic solution). When only one predictor is present, I need to reshape it to column form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1,1),yp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model has attributes \"coef_\", \"intercept_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept = model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the original and the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.scatter(x,yp, s = 20, c = 'gray', label = 'Data')\n",
    "plt.plot(x, slope*x + intercept , c ='k', label = 'Ordinary least squares fit')\n",
    "plt.plot(x, 3*x + 3, c = 'r', label = 'True regression line')\n",
    "plt.legend(fontsize = 14)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the analytic predictions for the coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions - fill in the analytic formula\n",
    "\n",
    "theta1 = \n",
    "\n",
    "theta0 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Theta_0, Theta_1:', theta0, theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can (and should!) do cross validation and all the nice things we have learned to do for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits = 5 , shuffle = True , random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(model, x.reshape(-1,1), yp, cv = cv, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.3f}'.format(scores['test_score'].mean()), '{:.3f}'.format(scores['test_score'].std()))\n",
    "print('{:.3f}'.format(scores['train_score'].mean()), '{:.3f}'.format(scores['train_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "\n",
    "- What are the scores that are being printed out? \n",
    "\n",
    "- How are the scores? \n",
    "\n",
    "- Does it suffer from high variance? High bias? \n",
    "\n",
    "- What would happen to the scores if we increased the scatter (noise)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Scoring in regression problems. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a way to visualize all the available scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(sklearn.metrics.SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you recognize some of them?\n",
    "\n",
    "Let's see if we can find the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(model, x.reshape(-1,1), yp, cv = cv, scoring = 'neg_mean_squared_error', return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('{:.3f}'.format(scores['test_score'].mean()), '{:.3f}'.format(scores['test_score'].std()))\n",
    "print('{:.3f}'.format(scores['train_score'].mean()), '{:.3f}'.format(scores['train_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might like to implement a scorer where we care about percentage error instead. Here is how to do a custom scorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(true,pred): #Modified Mean Absolute Percentage Error\n",
    "    return np.mean(np.abs((true-pred)/(0.5*(true+pred))))\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(model, x.reshape(-1,1), yp, cv = cv, scoring = mape_scorer, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.3f}'.format(scores['test_score'].mean()), '{:.3f}'.format(scores['test_score'].std()))\n",
    "print('{:.3f}'.format(scores['train_score'].mean()), '{:.3f}'.format(scores['train_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: as we already discussed, so far we have not changed the loss function (MSE), or the coefficients of the model. We have only looked at different evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'green'> Question 1: would the best fit line change if we optimize a different loss function? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'green'> Question 2: How can we implement that without an analytic solution? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can add some outliers to our data to make them more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12) #set \n",
    "out = np.random.choice(100,15) #select 15 outliers indexes\n",
    "yp_wo = np.copy(yp)\n",
    "np.random.seed(12) #set again\n",
    "yp_wo[out] = yp_wo[out] + 5*np.random.rand(15)*yp[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,yp_wo, label = 'Data + outliers')\n",
    "plt.scatter(x,yp, label = 'Original data')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the effect for the MSE loss right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1,1),yp_wo)\n",
    "\n",
    "slope, intercept = model.coef_, model.intercept_\n",
    "\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now implement the simplest form of gradient descent: batch, stochastic, and mini-batch, one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we add the bias term (a constant feature of value = 1); this is merely to write the prediction of the linear model in matrix multiplication form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((100, 1)), x] # add x0 = 1 to each instance; this is the bias term\n",
    "\n",
    "print(X.shape) #shape is number of instances x number of parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we save the coefficients and the loss for the normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ne = np.array([[...],[...]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ne = np.mean((X.dot(theta_ne) - yp_wo.reshape(-1,1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the gaps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10) #same initial conditions for all\n",
    "\n",
    "eta = \n",
    "n_iterations = \n",
    "N = 100 #number of points\n",
    "\n",
    "theta_path_bgd = []\n",
    "\n",
    "theta = #initialize how you like it!\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = \n",
    "    theta = \n",
    "    theta_path_bgd.append()\n",
    "\n",
    "theta_path_bgd = np.array(theta_path_bgd) #save the path\n",
    "\n",
    "theta_bgd = theta #final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "Are you finding the same coefficients and loss?\n",
    "\n",
    "What is the percentage difference with the loss derived from the normal equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you have time, you can also implement stochastic and/or mini-batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
