{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnh26cHKQv1q"
   },
   "source": [
    "### Random Forests and Photo-zs\n",
    "\n",
    "In this notebook, we use Random Forests to estimate photometric redshifts starting from observations of galaxy magnitudes in six different photometric bands (u, g, r, i, z, y). \n",
    "\n",
    "Copyright: Viviana Acquaviva (2023); see also other data credits below.\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)\n",
    "\n",
    "Essentially, we try to reproduce/improve upon the results of [this paper](https://arxiv.org/abs/1903.08174), for which the data are public and available [here](http://d-scholarship.pitt.edu/36064/). Additionally, we are very grateful to Jeff Newman for his expert advice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKiBCM6oQv1r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtgxhS0hQv1t"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-fEjYyuQv1u"
   },
   "outputs": [],
   "source": [
    "import astropy\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "#fits stands for Flexible Image Transport System; it's a format that allows one to store images and summary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUO-vGIKQv1v"
   },
   "source": [
    "#### Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TdqKz1KQv1v"
   },
   "source": [
    "We can read the data into a data frame using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8LTrJC-Qv1w"
   },
   "outputs": [],
   "source": [
    "with fits.open('DEEP2_uniq_Terapix_Subaru_v1.fits') as data:\n",
    "    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder()) #see https://numpy.org/devdocs/user/basics.byteswapping.html#changing-byte-ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEx0ecW2Qv1w",
    "outputId": "55b352b1-68f7-4499-eb00-873ab4cce770",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mlDqNP4Qv1x",
    "outputId": "a9635904-51d7-46ec-dc72-1755e10e58ad"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKaur7ZgQv1z",
    "outputId": "91eba0cd-af37-4a2b-a730-ab933fc6c24c"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Icbd4KqQv10"
   },
   "source": [
    "We can select the columns we want, corresponding to the brightness of the galaxies in the six bands of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdobujjcQv10"
   },
   "outputs": [],
   "source": [
    "features = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXMUfAkiQv11"
   },
   "source": [
    "The target property is the redshift. For this catalog, spectroscopic (more precise) redshifts, to use as the ground truth, are available in the column \"zhelio\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4Xo_lVmQv11"
   },
   "outputs": [],
   "source": [
    "target = df['zhelio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhxISFFXQv12",
    "outputId": "f6b89bda-b2bd-4234-f1c0-ec5b5aaad968"
   },
   "outputs": [],
   "source": [
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFy4Hvu8Qv12",
    "outputId": "c5e03ddc-051c-4674-85ee-ee3907bfb742"
   },
   "outputs": [],
   "source": [
    "target.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auB81H2uQv12"
   },
   "source": [
    "### Ok, we are now ready to run our first Random Forest model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DKxrrOfQv13"
   },
   "source": [
    "To get an idea of what we are shooting for, we can look at a figure in the paper:\n",
    "\n",
    " ![Performance of photometric redshift reconstruction](Photoz_RF_CFHTLS_Deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZym_077Qv13"
   },
   "source": [
    "In the figure above, $\\sigma_{NMAD}$ is the normalized median absolute deviation of the residual vector, and $\\eta$  is the fraction of outliers, defined as those objects for which (z_true - z_est)/(1+z_true) > 0.15.\n",
    "\n",
    "To be fair, we are working with DEEP2/3 data, so our range is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-HPiljuQv14"
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6C2JPqFQv14",
    "outputId": "3ffbe1c9-9c53-4f5f-9925-0f941375f3a4"
   },
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm0ChSvBQv14"
   },
   "source": [
    "We begin by establishing a benchmark (note: it takes a little time; ~35 secs on my machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlAIQmROQv15"
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(model, features, target, cv = KFold(n_splits=5, shuffle=True, random_state=10), return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-UYA4F7Qv15",
    "outputId": "f0e145f8-ff19-46ae-a160-790f119f6d84"
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsqB_ripQv15"
   },
   "source": [
    "As a reminder, the scores are the R2 score at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5XlSz2FQv15",
    "outputId": "f04d0be6-f905-4bde-d00e-f15872a09413"
   },
   "outputs": [],
   "source": [
    "np.mean(scores['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BS4FmVAQv16",
    "outputId": "28491343-d46b-449c-a841-d5c7dcaf8672"
   },
   "outputs": [],
   "source": [
    "np.mean(scores['train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2658KEPsQv16"
   },
   "source": [
    "Looks like we have a severe ..... issue! Let's also check the predictions and plot them against the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHCADREgQv16"
   },
   "outputs": [],
   "source": [
    "ypred = cross_val_predict(model, features, target, \\\n",
    "            cv = KFold(n_splits=5, shuffle=True, random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8PQeHm5Qv17",
    "outputId": "997df013-012e-4eef-9106-8dcc767f983a"
   },
   "outputs": [],
   "source": [
    "plt.scatter(target,ypred, s = 20, c = 'royalblue')\n",
    "plt.xlabel('True (spectroscopic) z', fontsize=14)\n",
    "plt.ylabel('Predicted z',fontsize=14)\n",
    "plt.axis('square')\n",
    "plt.xlim(0,3)\n",
    "plt.ylim(0,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt55-q07Qv17"
   },
   "source": [
    "### Question: Does it look like the one of the paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktMqM6jtQv17"
   },
   "source": [
    "It's also interesting to look at the distribution of the predicted values, to see how they always tend to produce a narrower distribution. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jja8vHCpQv18",
    "outputId": "ed1e5aad-a73d-4ab3-c989-e9c8b61134fc"
   },
   "outputs": [],
   "source": [
    "plt.hist(target,bins=50,density=False,alpha=0.5, range = (0,3), label = 'True');\n",
    "plt.hist(ypred,bins=50,density=False,alpha=0.5, range = (0,3), color = 'g', label = 'Predicted');\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a characteristic of tree-based methods; examples with very low/high values of the target are \"bunched up\" with others that have higher/lower values, so the predictions tend to be over-estimated for low values of the target variables and under-estimated for high values. You can usually see this trend in scatter plots as well! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjolDcK-Qv18"
   },
   "source": [
    "Ok, we are now ready to calculate the outlier fraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5VmH8GOQv18",
    "outputId": "f3c7681d-ea8f-4590-9624-4c25dddde8f1"
   },
   "outputs": [],
   "source": [
    "len(np.where(np.abs(target-ypred)>0.15*(1+target))[0])/len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ataKR4VuQv19"
   },
   "source": [
    "And NMAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHTAo8pNQv19",
    "outputId": "6cc06861-7b68-4d2a-ba3a-d6d1439216e7"
   },
   "outputs": [],
   "source": [
    "1.48*np.median(np.abs(target-ypred)/(1 + target)) \n",
    "#The 1.48 is there because for a Gaussian distribution, this becomes the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V77W36MQv19"
   },
   "source": [
    "### Clearly, we are very far from the paper's performance :( \n",
    "\n",
    "### Because we have a pretty severe high variance issue, we can perform some parameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbDXjDhRQv19"
   },
   "source": [
    "We can start by making the data set a bit smaller, as we have seen that timings were already challenging in simple k-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoDn2jaxQv1-"
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "sel = np.random.choice(range(len(ypred)), 5000, replace = False) #sample without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LznFMwt7Qv1-",
    "outputId": "d6b4b347-ad04-46e2-bd63-2bcbb8931b88"
   },
   "outputs": [],
   "source": [
    "len(np.unique(sel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ld3W4UOQv1-"
   },
   "source": [
    "And we create our new smaller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O18fZWV5Qv1-"
   },
   "outputs": [],
   "source": [
    "seld = features.loc[sel,:]\n",
    "selt = target[sel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzH4ck7_W1nk"
   },
   "source": [
    "It is good practice to ensure that the performance on the smaller set remains similar to the one obtained on the entire data set, which means that the change in size will not significantly affect the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yPWNGKYQv1-"
   },
   "outputs": [],
   "source": [
    "littlescores = cross_validate(model,seld,selt, cv = KFold(n_splits=5, shuffle=True, random_state=10), return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LstMVPAzQv1_",
    "outputId": "54ff87cc-dad7-4ad5-b6fe-807b0a4a7398"
   },
   "outputs": [],
   "source": [
    "littlescores['test_score'].mean(), littlescores['train_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgU0AJtIQv1_"
   },
   "source": [
    "### We are now ready to optimize hyperparameters - what should we choose?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qx8IihvwQv1_"
   },
   "source": [
    "#### Tree Parameters\n",
    "\n",
    "Some useful parameters associated to a tree are:\n",
    "\n",
    "-  The minimum number of instances in a leaf node;\n",
    "\n",
    "-  The minimum number of instances required in a split node;\n",
    "\n",
    "- The maximum depth of tree;\n",
    "\n",
    "-  The criterion chosen to decide whether a split is \"worth it\", expressed in terms of information gain.\n",
    "\n",
    "\n",
    "#### Randomization Parameters\n",
    "\n",
    "Here we find:\n",
    "\n",
    "- The number of k < n features that are used in building trees;\n",
    "\n",
    "- The re-sampling (boostrap) of the data set (T or F).\n",
    "\n",
    "\n",
    "#### Forest Parameters\n",
    "\n",
    "The number of trees in the forest (n_estimators) can be adjusted, with the general understanding that more trees are better, but at some point performance will plateau, so one can find the trade-off between having more trees and lower runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUk2q6d4XXrP"
   },
   "source": [
    "We can visualize them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CezgP0qQv1_",
    "outputId": "b1192f39-a66c-4f1c-8db5-f3c090ef985e"
   },
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wbIVzT7Qv2A"
   },
   "source": [
    "Here below is a possible set; we can run a grid search to look for the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYGWEz_qQv2A"
   },
   "source": [
    "- min_impurity_decrease \n",
    "\n",
    "- number of trees\n",
    "\n",
    "- max_leaf_nodes\n",
    "\n",
    "- min_samples_split\n",
    "\n",
    "- max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bi1FmG7pQv2B",
    "outputId": "0ac959f4-be3c-43e0-dd4f-7f56cff91ac3"
   },
   "outputs": [],
   "source": [
    "#Takes a few minutes\n",
    "\n",
    "parameters = {'min_impurity_decrease':[0.1, 0.5, 0.0], \\\n",
    "              'max_features':[None,4,2], 'n_estimators':[50, 100, 200], 'min_samples_split': [10,20,100], \n",
    "              'max_leaf_nodes':[None, 100, 200]}\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(RandomForestRegressor(), parameters, cv = KFold(n_splits=5, shuffle=True), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(seld,selt)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGcU8HO-Z5UF"
   },
   "source": [
    "As usual, we save the results in a data frame, and look at the best models to build some intution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8vzC3wyQv2B"
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvQOiu2QQv2B",
    "outputId": "71dd18d7-b3bd-4acf-edb6-91048888b7cf"
   },
   "outputs": [],
   "source": [
    "scoresCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaPY_68QQv2B"
   },
   "source": [
    "### And the verdict is...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCLdKDhyQv2C"
   },
   "source": [
    "We have NOT improved the test scores.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zxsWtClQv2C"
   },
   "source": [
    "### Before giving up, and after noting that our issue is so sever that it's hard to attribute it to an optimization or chooice of algorithm issue, we need to look more carefully at data cleaning and/or imputing (something, in fact, that should be the first step in our pipeline!)\n",
    "\n",
    "In my case, it was time to write to the authors of the paper, who told me exactly how they selected the data that went into making the learning set.\n",
    "\n",
    "We needed to retain some additional columns to be used in the selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StcRSEwGQv2C"
   },
   "outputs": [],
   "source": [
    "mags = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor','subaru_source','cfhtls_source','zquality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4mPKRehQv2C",
    "outputId": "dc01e66b-d55e-4653-d66e-8d11797bc95a"
   },
   "outputs": [],
   "source": [
    "mags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4COvGDwQv2C",
    "outputId": "b3463f9a-21e4-407d-bcbd-f233545ffe8a"
   },
   "outputs": [],
   "source": [
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojJL3glYQv2C",
    "outputId": "57b1a547-3031-4c77-9c14-32bb33be3b23"
   },
   "outputs": [],
   "source": [
    "#redshift quality - only use objects with high-quality spectroscopic redshift measurements\n",
    "\n",
    "mags = mags[mags['zquality'] >= 3]\n",
    "\n",
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM4en7AaQv2C",
    "outputId": "11e7a92d-5fd3-41e8-ace1-4f36eaa069c1"
   },
   "outputs": [],
   "source": [
    "#select objects with cfhtls deep photometric data \n",
    "\n",
    "mags = mags[mags['cfhtls_source'] == 0]\n",
    "\n",
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zc5FJlieQv2D"
   },
   "outputs": [],
   "source": [
    "#This additional selection (objects with subaru deep photometry) was irrelevant\n",
    "\n",
    "#mags = mags[mags['subaru_source'] == 0]\n",
    "\n",
    "#mags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHcinRRWQv2D"
   },
   "source": [
    "Unavailable measurements are marked by -99 or 99 (while typical values are around 20-25). We also get rid of data with missing measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxIykm0mQv2D"
   },
   "outputs": [],
   "source": [
    "mags = mags[mags > -10].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5_bEXixQv2D",
    "outputId": "243e22ab-0943-40d3-f579-fb3d50123a48"
   },
   "outputs": [],
   "source": [
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuSKRxP0Qv2D"
   },
   "outputs": [],
   "source": [
    "mags = mags[mags < 90].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvMNdxoBQv2E",
    "outputId": "b44d8360-a06e-4e39-e423-504a9b115975"
   },
   "outputs": [],
   "source": [
    "mags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwPbGAZ6Qv2E"
   },
   "source": [
    "Our final set is made of the six original features and it has 6,307 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJ85PSg0Qv2E",
    "outputId": "63d08e83-12ad-455d-e1a5-0461365150c7"
   },
   "outputs": [],
   "source": [
    "sel_features = mags[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']]\n",
    "sel_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7rQEb-_Qv2E"
   },
   "source": [
    "We need, of course, to select the same set on the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_e1CE86Qv2E"
   },
   "outputs": [],
   "source": [
    "sel_target = target[sel_features.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_diGu-rQv2F"
   },
   "source": [
    "Let's see how our benchmark model works. Note that for reproducible results we need to fix the random\\_state parameter of the Random Forest (which controls the bootstrap process) and the random seed of the cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgGN-LvbQv2F"
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(RandomForestRegressor(random_state = 5),sel_features,sel_target,cv = KFold(n_splits=5, shuffle=True, random_state=10), \\\n",
    "               return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRdK3chRQv2F",
    "outputId": "44b6486a-c441-462d-a4de-562237ffb7a8"
   },
   "outputs": [],
   "source": [
    "print(np.round(np.mean(scores['test_score']),3), np.round(np.std(scores['test_score']),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5pIMGnvQv2F",
    "outputId": "e1566245-a50b-4b57-871c-01d1b365efba"
   },
   "outputs": [],
   "source": [
    "print(np.round(np.mean(scores['train_score']),3), np.round(np.std(scores['train_score']),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biGkjPG6Qv2F"
   },
   "source": [
    "The scores have improved noticeably! However, we can still observe high variance. We can re-run the optimization process (note that the data set size is limited, so we don't need to make it smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jOl8WxkQv2G",
    "outputId": "785a3623-5db2-4844-e103-9e3c58ea00df"
   },
   "outputs": [],
   "source": [
    "#This now takes ~3 minutes\n",
    "\n",
    "parameters = {'max_depth':[3, 6, None], \\\n",
    "              'max_features':[None,4,2], 'n_estimators':[50,100,200], 'min_samples_leaf': [1,5,10]}\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(RandomForestRegressor(random_state = 5), parameters, cv = KFold(n_splits=5, shuffle=True, random_state=10), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(sel_features,sel_target)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehogMrLMQv2G",
    "outputId": "49930cd1-4204-4a64-97e5-37c01af4b036"
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n",
    "scoresCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmJTnxaPQv2G"
   },
   "source": [
    "### Discussion Question: \n",
    "\n",
    "#### Do we need to explore some parameters more in detail (i.e., do we expect a significant improvement by enlarging the space of parameters?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48v5xfQUQv2G"
   },
   "outputs": [],
   "source": [
    "bm = model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UXfvfPlQv2H"
   },
   "source": [
    "We can generate one set of predictions to visualize what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZ3M2t8EQv2H"
   },
   "outputs": [],
   "source": [
    "ypred = cross_val_predict(bm, sel_features,sel_target, cv = KFold(n_splits=5, shuffle=True, random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HV0EBv-Qv2H",
    "outputId": "18f27073-ccdc-4d72-b483-589e126379b1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(sel_target,ypred, s =10)\n",
    "plt.xlabel('z_spec')\n",
    "plt.ylabel('z_photo')\n",
    "plt.ylim(0,2)\n",
    "plt.xlim(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb4G55BkQv2H"
   },
   "source": [
    "Calculate the outlier fraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhSx8Y06Qv2H",
    "outputId": "604d3160-cbea-4890-bc1a-83fdf9deffc9"
   },
   "outputs": [],
   "source": [
    "len(np.where(np.abs(sel_target-ypred)>0.15*(1+sel_target))[0])/len(sel_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIyvL3VjQv2I"
   },
   "source": [
    "Calculate Normalized Median Absolute Deviation (NMAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzU_M7hOQv2I",
    "outputId": "c4a42758-693a-475d-a38e-217b29d7720e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1.48*np.median(np.abs(sel_target-ypred)/(1 + sel_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxUX4w7hQv2I"
   },
   "source": [
    "### Conclusion: How does our model compare with the paper's results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEyoP9BVQv2I"
   },
   "source": [
    "### Can you think of a feature engineering exercise that can be helpful? (if you need a hint: it's in Section 7 of the paper ;) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
